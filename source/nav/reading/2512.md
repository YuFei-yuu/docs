# 12月记录

## 前置理解

纯入门蛐蛐几天零散的个人理解，大概率有很多问题，先记着（）

毕竟是相当于接触一个比较新的领域，个人还是比较习惯类比理解，对这个领域在做什么、需要解决什么问题有一个整体的把握。

如果说以前在做的传统机器人是在ubuntu下基于ros2提供的框架，将各种定位、重定位等算法模块拼凑成一个系统框架实现所需的功能，那么现在的具身vla等大概就是利用pytorch库提供的框架，结合transformers库中提供的各种ai模型，搭建各种模块实现期望的功能？

感觉其实从传统激光雷达SLAM到具身vla这些，本质上的目标大差不差，都是在解决同样的问题，整体依旧是建图、定位、感知、规划、决策这些老生常谈的东西。最核心的区别大概是从以前更加几何、更“算法”的方法，变成了现在需要语义、常识等结合了learning的方法。

从这个角度来说，一大问题就是我们几何计算的结果是硬约束的、确定的，而learning得到的结果往往是“软”的、概率性的，所以目前看下来整体在解决的问题就是，我们如何通过各种方式能够更大概率地在各个环节得到期望的结果。

- 建图

    都具身了基本就都是无先验地图的导航，那么需要解决的一大问题就是如何在探索的过程中完成高效的地图构建。目前看下来主要有两个问题：
    1. 怎么把传感器收集到的信息更高效的与语义联系起来
    2. 怎么把这些语义信息表示为地图
   
- 决策
    
    既然无先验了，那本来在传统slam里目标点是一个确定的map系下的坐标，但现在不知道他在哪里了，那还怎么选定一个“更可能找到目标”的路径呢？那显然一次性规划成功是不现实的，一个中间点一个中间点去乱找也是不现实的，那怎么能设计一个更高效的探索策略呢？


## VLFM

主要内容： 
- 零样本语义目标导航（Zero-Shot ObjectNav）：如何让机器人在没有任何预训练、没有地图、对环境一无所知的情况下，仅凭一个单词给出的目标就能在陌生的室内环境中找到这个东西
- 价值地图（Value Map）：算是这篇的核心，用视觉语言模型BLIP-2把图像和目标联系起来，计算得到一个相似度分数，然后把这个分数投影到栅格地图上，形成一个价值地图，表示每个位置找到目标的可能性
- 规划方式：基于价值地图进行路径规划，选择那些价值高的区域进行探索，检测到目标之后就点对点导航过去
  
问题：
- 价值地图根据当前目标构建，更换目标不可复用，每一次探索几乎都是全新的，实际上并不算记录了过程中探索得到的信息
- 不知道是不是我没看懂，这个相似性分数就全靠模型告诉你吗，没有做更多的逻辑判断，对比下面场景图的构建这个问题就更明显了，但感觉把分数投在栅格图上的处理方法还是挺新鲜的 ~~（废话第一篇看的论文看到什么不新鲜）~~

## SG-nav

感觉最主要的贡献是把整个环境都共同用语义表示出来了，把从房间到一个区域再到一个具体的物体之间的结构表示出来了，相当于能够利用上周边场景的信息做推理，地图不仅仅用来做定位和路径规划了，同时也是分析决策的依据。

问题：
感觉有问题的点都有点像挑刺（）
- Groop组的分割靠LLM预先构建的一个字典，告诉你什么和什么关联，感觉有点刻板
- 长时间更新时场景图维护问题，直觉上这个数据量不会小
- 最开始的room识别错了对后续的影响会很大


## CogNav
核心是由LLM驱动的状态机，主要解决的问题还是怎么选目标，把神经科学里人类怎么找东西的心路历程加到导航里了，在真的就是要确定一个目标之前会先有状态判断：
- Exploration：主要任务是探索，找可能是目标的东西
- Exploitation：发现疑似是目标的东西，走近确认看看
- Confirmation：确认这就是目标之后任务结束
对比VLFM认为目标置信度大于一个阈值就直接冲过去，CogNav的优势或许是在于对于一个很像目标的假目标更有能把自己从牛角尖里拽回来的能力？导航到目标之后会进行检测，检测出来不是能再重新切回探索状态。感觉创新点并不在行为本身，而是明确了其中切换的过程 ~~毕竟真论起来的话什么算法不是先找找到了再过去~~

问题：让LLM能准确判断状态应该存在难度，从探索到利用之间的界限并不分明，对prompt有要求

## UniGoal
和SGnav同一个团队，在sgnav的基础上吧目标也表示成了图的结构，把以往互相割裂的以特定物体/类别为目标/以一个描述为目标/以一个图片为目标全都用VLM和LLM统一表示成Goal Graph的形式，找目标实际上就是找这个图在场景图中结构匹配的子图

~~感觉看到了故人汁汁……似李！点云配准！~~

和点云配准还是不太一样。最初有个疑惑就是场景图也是LLM得出的，目标图也LLM得出的，那LLM的理解不一致结构不一样不就配不上了吗？再读才理解这个匹配也是LLM在干，且也设计了包容部分匹配的容错机制，匹配也是在看向量空间里的相似度，而不是严格的图论意义上的子图同构

还是思维不够LLM啊 :（

## PONI
没什么learning底子读这篇略感吃力，或许算得上是VLFM的前身？

大概就是用俯视的户型图训练，让模型能够推测在一个局部视角下哪个方向更可能有新空间、哪个方向更可能有目标，然后在实际导航任务中把机器人丢进去，建出来一张局部图，把这个局部图喂给模型，让模型根据更能探索到新空间和更能找到目标打分，分高地方的就选做目标继续去探索

毕竟很老了问题也很明显，相比后面直接用LLM需要预训练，也不够语义，对物体关系的理解也停留在静态的看到了A在B旁边这种，没有更深的逻辑推理能力

## Imagine Before Go
开天眼了，构建出局部地图之后用生成式模型脑补出来一个完整的语义地图，用这个完整的地图进行规划，不再需要走一步看一步走一步看一步这样一点点试

非常的新奇但感觉问题也非常多：
- 需要预训练一个生成式模型用来画地图
- 生成式模型的不靠谱想必大家心里也都有数（）
- 对环境的理解也依旧停留在比较简单的A挨着B这种层次
- 看起来很依赖局部地图的大小和质量
