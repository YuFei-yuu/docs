# 12月记录

## 前置理解

纯入门蛐蛐几天零散的个人理解，大概率有很多问题，先记着（）

毕竟是相当于接触一个比较新的领域，个人还是比较习惯类比理解，对这个领域在做什么、需要解决什么问题有一个整体的把握。

如果说以前在做的传统机器人是在ubuntu下基于ros2提供的框架，将各种定位、重定位等算法模块拼凑成一个系统框架实现所需的功能，那么现在的具身vla等大概就是利用pytorch库提供的框架，结合transformers库中提供的各种ai模型，搭建各种模块实现期望的功能？

感觉其实从传统激光雷达SLAM到具身vla这些，本质上的目标大差不差，都是在解决同样的问题，整体依旧是建图、定位、感知、规划、决策这些老生常谈的东西。最核心的区别大概是从以前更加几何、更“算法”的方法，变成了现在需要语义、常识等结合了learning的方法。

从这个角度来说，一大问题就是我们几何计算的结果是硬约束的、确定的，而learning得到的结果往往是“软”的、概率性的，所以目前看下来整体在解决的问题就是，我们如何通过各种方式能够更大概率地在各个环节得到期望的结果。

- 建图

    都具身了基本就都是无先验地图的导航，那么需要解决的一大问题就是如何在探索的过程中完成高效的地图构建。目前看下来主要有两个问题：
    1. 怎么把传感器收集到的信息更高效的与语义联系起来
    2. 怎么把这些语义信息表示为地图
   
- 决策
    
    既然无先验了，那本来在传统slam里目标点是一个确定的map系下的坐标，但现在不知道他在哪里了，那还怎么选定一个“更可能找到目标”的路径呢？那显然一次性规划成功是不现实的，一个中间点一个中间点去乱找也是不现实的，那怎么能设计一个更高效的探索策略呢？


## VLFM

主要内容： 
- 零样本语义目标导航（Zero-Shot ObjectNav）：如何让机器人在没有任何预训练、没有地图、对环境一无所知的情况下，仅凭一个单词给出的目标就能在陌生的室内环境中找到这个东西
- 价值地图（Value Map）：算是这篇的核心，用视觉语言模型BLIP-2把图像和目标联系起来，计算得到一个相似度分数，然后把这个分数投影到栅格地图上，形成一个价值地图，表示每个位置找到目标的可能性
- 规划方式：基于价值地图进行路径规划，选择那些价值高的区域进行探索，检测到目标之后就点对点导航过去
  
问题：
- 价值地图根据当前目标构建，更换目标不可复用，每一次探索几乎都是全新的，实际上并不算记录了过程中探索得到的信息
- 不知道是不是我没看懂，这个相似性分数就全靠模型告诉你吗，没有做更多的逻辑判断，对比下面场景图的构建这个问题就更明显了，但感觉把分数投在栅格图上的处理方法还是挺新鲜的 ~~（废话第一篇看的论文看到什么不新鲜）~~

## SG-nav

感觉最主要的贡献是把整个环境都共同用语义表示出来了，把从房间到一个区域再到一个具体的物体之间的结构表示出来了，相当于能够利用上周边场景的信息做推理，地图不仅仅用来做定位和路径规划了，同时也是分析决策的依据。

问题：
感觉有问题的点都有点像挑刺（）
- Groop组的分割靠LLM预先构建的一个字典，告诉你什么和什么关联，感觉有点“刻板”
- 长时间更新时场景图维护问题，直觉上这个数据量不会小
- 最开始的room识别错了对后续的影响会很大


## tbc
（实则是妹时间写了啊啊啊以后再补